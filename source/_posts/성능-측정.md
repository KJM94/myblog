---
title: 성능 측정
date: 2021-04-17 15:40:38
tags: python
---

# 성능 측정

- 정확도
- 혼동 행렬
- 정밀도
- 재현율
- F1 Score
- ROC curve
- AUC

## 정확도(Accuracy)

- 전체 값 중에 올바르게 예측한 값이 몇 개인지 판단
- 직관적으로 모델 예측 성능을 나타내는 평가 지표

예측결과가 동일한 데이터 건수 / 전체 예측 데이터 건수

## 혼동 행렬(Confusion matrix)

- 모델의 성능을 평가할 때 사용되는 지표
- 예측 값이 실제 관측 값을 얼마나 정확하게 예측 했는지 보여주는 행렬

## 정밀도(Precision)

- 모델의 예측 값이 얼마나 정확하게 예측됐는가를 나타내는 지표
- 관측의 균질성
- 관측된 값의 편차가 적을수록 정밀

TP/TP+FP

## 재현율(recall)

- 실제 값 중에서 모델이 검출한 실제 값의 비율을 나타내는 지표

TP/TP+FN

## F1 Score

- 정밀도와 재현율 두 값을 조화 평균을 내서 하나의 수치로 나타낸 지표

2 * 재현율 * 정밀도/(재현율+정밀도)

## 정밀도/ 재현율 트레이드오프

- 정밀도와 재현율의 중요성은 사례마다 다름
- 임곗값(Threshold)를 조정하여 정밀도나 재현율의 수치 조정
- 임곗값이 높을수록 재현율을 낮아지고 정밀도는 높아짐
- 정밀도와 재현율은 서로 반비례 관계

## ROC curve

- 임곗값(threshold)이 달라짐에 따라 분류모델의 성능이 어떻게 변하는지를 나타내는 곡선
- 임곗값의 변화에 따라 성능 평가 지표의 값이 어떻게 변하는지를 시각화한 곡선

- 분류 모델의 y값은 기본적으로 확률 값으로 출력 이를 분류 결과로 변환해 주기 위해서는 0과 1사이의 일정한 값을 기준으로 이보다 크면 참, 작으면 거짓으로 취급함

임곗값을 달리 취해주는 경우

- 일반적으로는 0.5를 기준으로 사용하지만, 참인 것을 반드시 잡아주어야 하는 경우 기준값 변경

- 환자가 코로나 바이러스와 같은 전염병에 걸렸는지 여부, 조금이라도 낌새가 있으면 모두 양성으로 처리해야 할 때를 예로 들 수 있음

- False Negative를 최대한 피해 주어야 할 경우 임곗값을 0.1로 잡아준다던지, 그 반대의 경우로 최대한 참으로 판명하는 것을 보수적으로 결정해야 할 경우에는 0.9로 잡아 주는 등의 결정을 내려야함

- 기준 값을 낮추면 그에 따라 Positive 예측 확률이 증가하고 그에 따라서 재현율이 증가하게 되어 대상 물체를 빠뜨리지 않고 잘 잡아내게 됨

- 반대로 기준값을 높이면 정밀도가 증가하여 검출된 결과가 얼마나 정확한지 다시 말하여 검출 결과들 중 실제 물체가 얼마나 포함되어 있는지를 나타냄

- 분석가가 수용 가능한 False Positive Rate 정도를 결정해 주어야 함

## AUC

Area Under the Curve

- ROC 커브 하단 영역의 넓이를 구한 값 0~1 사이의 값을 갖는다 더 높을수록 더 좋은 분류 성능
- 판단이 불분명한 부분이 적을수록 이상적인 분류 성능을 보임